{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769fa1d5-df6c-4526-bd8d-08cee06cc9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 13:59:07 WARN Utils: Your hostname, OSSalwa resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/23 13:59:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/23 13:59:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb5d140-c1ce-4592-9cef-c6c1da5f2c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 13:59:24 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/11/23 13:59:33 WARN Instrumentation: [8131e6af] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/11/23 13:59:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/23 13:59:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/11/23 13:59:39 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "[Stage 1:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0fbbe8-e3c9-4125-b4e6-e89c6fa26951",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column Features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train logistic regression model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display coefficients and summary\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoefficients: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mcoefficients\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data_19/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data_19/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data_19/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data_19/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/big_data_19/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column Features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>."
     ]
    }
   ],
   "source": [
    "# Practice: Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Example dataset\n",
    "data = [(1, [2.0, 3.0], 0), (2, [1.0, 5.0], 1), (3, [2.5, 4.5], 1), (4, [3.0, 6.0], 0)]\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Display coefficients and summary\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42230020-6066-4a5c-b7a8-f9215e820346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057891021882,4.087352253767593]\n",
      "Intercept: 11.568912688492174\n"
     ]
    }
   ],
   "source": [
    "# (perbaikan) Practice: Logistic Regression\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Example dataset\n",
    "data = [\n",
    "    (1, 2.0, 3.0, 0),\n",
    "    (2, 1.0, 5.0, 1),\n",
    "    (3, 2.5, 4.5, 1),\n",
    "    (4, 3.0, 6.0, 0)\n",
    "]\n",
    "\n",
    "columns = ['ID', 'F1', 'F2', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['F1', 'F2'],\n",
    "    outputCol='Features'\n",
    ")\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Display coefficients and summary\n",
    "print(\"Coefficients:\", model.coefficients)\n",
    "print(\"Intercept:\", model.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "723df7f5-f749-417d-bca7-4342dd7f8837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([3., 3.]), array([12.5, 12.5])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "data = [\n",
    "    (1, 1.0, 1.0),\n",
    "    (2, 5.0, 5.0),\n",
    "    (3, 10.0, 10.0),\n",
    "    (4, 15.0, 15.0)\n",
    "]\n",
    "\n",
    "columns = ['ID', 'x', 'y']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['x', 'y'],\n",
    "    outputCol='Features'\n",
    ")\n",
    "\n",
    "df2 = assembler.transform(df)\n",
    "\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df2)\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers:\", centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843d0e4-15bc-4626-9cf4-05d67ffa7c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "001084bc-faea-45ae-8a81-b9837599d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOMEWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b67c81ea-a2a5-4600-921f-f5e4d3cb8cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 14:48:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 baris pertama dataset:\n",
      "+-----------+--------+---------------------------------------------------+-------------------+-----+-----+----------------+-------------------+--------+----------+----------+--------+--------+\n",
      "|PassengerId|Survived|Name                                               |Age                |SibSp|Parch|Ticket          |Fare               |Sex_male|Embarked_Q|Embarked_S|Pclass_2|Pclass_3|\n",
      "+-----------+--------+---------------------------------------------------+-------------------+-----+-----+----------------+-------------------+--------+----------+----------+--------+--------+\n",
      "|1          |0       |Braund, Mr. Owen Harris                            |0.37500000000000006|1    |0    |A/5 21171       |0.11046036834342966|true    |false     |true      |false   |true    |\n",
      "|2          |1       |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|0.6826923076923077 |1    |0    |PC 17599        |1.0                |false   |false     |false     |false   |false   |\n",
      "|3          |1       |Heikkinen, Miss. Laina                             |0.4519230769230769 |0    |0    |STON/O2. 3101282|0.12074460953402483|false   |false     |true      |false   |true    |\n",
      "|4          |1       |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |0.625              |1    |0    |113803          |0.8090269736601539 |false   |false     |true      |false   |false   |\n",
      "|5          |0       |Allen, Mr. William Henry                           |0.625              |0    |0    |373450          |0.12264909864339432|true    |false     |true      |false   |true    |\n",
      "+-----------+--------+---------------------------------------------------+-------------------+-----+-----+----------------+-------------------+--------+----------+----------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Schema dataset:\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Sex_male: boolean (nullable = true)\n",
      " |-- Embarked_Q: boolean (nullable = true)\n",
      " |-- Embarked_S: boolean (nullable = true)\n",
      " |-- Pclass_2: boolean (nullable = true)\n",
      " |-- Pclass_3: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inisialisasi Spark\n",
    "spark = SparkSession.builder.appName(\"Week14_MLlib\").getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "df = spark.read.csv(\"file:///home/salwa/Downloads/titanic_clean.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"5 baris pertama dataset:\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nSchema dataset:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8adad58-7a7a-4a82-a737-38af77fd8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hapus kolom yang tdk diperlukan\n",
    "\n",
    "df = df.drop(\"Name\", \"Ticket\", \"PassengerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16457eb7-88b8-4522-887a-031c608586ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubah kolom survived menjadi label\n",
    "df = df.withColumnRenamed(\"Survived\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e65a55c5-0120-4e62-b5c2-e00284fd5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tentukan kolom fitur\n",
    "\n",
    "feature_cols = [\n",
    "    \"Age\", \"SibSp\", \"Parch\", \"Fare\",\n",
    "    \"Sex_male\", \"Embarked_Q\", \"Embarked_S\",\n",
    "    \"Pclass_2\", \"Pclass_3\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbbc127a-92da-4341-a561-da058fcc36a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data setelah VectorAssembler:\n",
      "+---------------------------------------------------------------------+-----+\n",
      "|features                                                             |label|\n",
      "+---------------------------------------------------------------------+-----+\n",
      "|[0.37500000000000006,1.0,0.0,0.11046036834342966,1.0,0.0,1.0,0.0,1.0]|0    |\n",
      "|(9,[0,1,3],[0.6826923076923077,1.0,1.0])                             |1    |\n",
      "|(9,[0,3,6,8],[0.4519230769230769,0.12074460953402483,1.0,1.0])       |1    |\n",
      "|(9,[0,1,3,6],[0.625,1.0,0.8090269736601539,1.0])                     |1    |\n",
      "|[0.625,0.0,0.0,0.12264909864339432,1.0,0.0,1.0,0.0,1.0]              |0    |\n",
      "+---------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VectorAssembler\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df2 = assembler.transform(df).select(\"features\", \"label\")\n",
    "\n",
    "print(\"Data setelah VectorAssembler:\")\n",
    "df2.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee32cbd3-8545-4fb0-ac92-38d80038e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train-Test\n",
    "\n",
    "train, test = df2.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3c64a77-39f4-4af6-8419-166f1ba43b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a classification model using Spark MLlib and evaluate its performance.\n",
    "# Explore hyperparameter tuning using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fce08bf7-794f-4012-a34b-562b70995594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# MODEL 1: Random Forest Classifier\n",
    "\n",
    "# train random forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=50\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train)\n",
    "rf_pred = rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5640afc-1718-4dfb-b049-011908c99f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Random Forest: 0.8933\n"
     ]
    }
   ],
   "source": [
    "# evaluasi random forest\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "rf_auc = evaluator.evaluate(rf_pred)\n",
    "print(f\"AUC Random Forest: {rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c03241d-11b9-40be-8ccb-1b8123e36c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Random Forest berjalan...\n",
      "AUC Random Forest (after CV): 0.8726\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "rf_paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(rf.numTrees, [30, 50, 80])\n",
    "                .addGrid(rf.maxDepth, [3, 5, 8])\n",
    "                .build())\n",
    "\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=rf_paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "print(\"Cross Validation Random Forest berjalan...\")\n",
    "rf_cv_model = rf_cv.fit(train)\n",
    "\n",
    "rf_cv_pred = rf_cv_model.transform(test)\n",
    "rf_cv_auc = evaluator.evaluate(rf_cv_pred)\n",
    "\n",
    "print(f\"AUC Random Forest (after CV): {rf_cv_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1c24be6-3ab1-428a-9607-27c977c19154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters (Random Forest)\n",
      "numTrees : 30\n",
      "maxDepth : 8\n"
     ]
    }
   ],
   "source": [
    "# best parameter random forest\n",
    "\n",
    "best_rf = rf_cv_model.bestModel\n",
    "\n",
    "print(\"\\nBest Parameters (Random Forest)\")\n",
    "print(\"numTrees :\", best_rf.getNumTrees)\n",
    "print(\"maxDepth :\", best_rf.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "147f7ed8-fb20-4e5a-98f6-8fa5948902fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59f761b0-d55a-4aa5-b50a-890dffa03067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2: Logistic Regression\n",
    "\n",
    "# Train Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=50\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train)\n",
    "lr_pred = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09db0dff-e3ae-4af3-adfa-b38c84168a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Logistic Regression: 0.8870\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi Logistic Regression\n",
    "\n",
    "lr_auc = evaluator.evaluate(lr_pred)\n",
    "print(f\"AUC Logistic Regression: {lr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33e2b36a-f59e-4f07-be5c-3e9bfdb427af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Logistic Regression berjalan...\n",
      "AUC Logistic Regression (after CV): 0.8916\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning Logistic Regression\n",
    "\n",
    "lr_paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "                .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                .build())\n",
    "\n",
    "lr_cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=lr_paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "print(\"Cross Validation Logistic Regression berjalan...\")\n",
    "lr_cv_model = lr_cv.fit(train)\n",
    "\n",
    "lr_cv_pred = lr_cv_model.transform(test)\n",
    "lr_cv_auc = evaluator.evaluate(lr_cv_pred)\n",
    "\n",
    "print(f\"AUC Logistic Regression (after CV): {lr_cv_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2e3e5da-8f11-4c59-9a39-bcc58a69e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters (Logistic Regression):\n",
      "regParam : 0.01\n",
      "elasticNetParam : 0.5\n"
     ]
    }
   ],
   "source": [
    "# Best Parameters Logistic Regression\n",
    "\n",
    "best_lr = lr_cv_model.bestModel\n",
    "\n",
    "print(\"\\nBest Parameters (Logistic Regression):\")\n",
    "print(\"regParam :\", best_lr._java_obj.getRegParam())\n",
    "print(\"elasticNetParam :\", best_lr._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f2a0d-4cc4-4492-be16-f35e91a4e341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
